{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11 - Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "+ to do regression using a GP\n",
    "+ to find the hyperparameters of the GP by maximizing the (marginal) likelihood\n",
    "+ to use GP regression for uncertainty propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "+ Please read [this](http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2903.pdf) OR watch [this video lecture](http://videolectures.net/mlss03_rasmussen_gp/?q=MLSS).\n",
    "\n",
    "+ [Section 5.4 in GP for ML textbook](http://www.gaussianprocess.org/gpml/chapters/RW5.pdf).\n",
    "\n",
    "+ See slides for theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The purpose of this example is to demonstrate Gaussian process regression. To motivate the need let us introduce a toy uncertainty quantification example:\n",
    "\n",
    "> We have developed an \"amazing code\" that models an extremely important physical phenomenon. The code works with a single input paramete $x$ and responds with a single value $y=f(x)$. A physicist, who is an expert in the field, tells us that $x$ must be somewhere between 0 and 1. Therefore, we treat it as uncertain and we assign to it a uniform probability density:\n",
    "$$\n",
    "p(x) = \\mathcal{U}(x|0,1).\n",
    "$$\n",
    "Our engineers tell us that it is vitally important to learn about the average behavior of $y$. Furthermore, they believe that a value of $y$ greater than $1.2$ signifies a catastrophic failure. Therefore, we wish to compute:\n",
    "1. the variance of $y$:\n",
    "$$\n",
    "v_y = \\mathbb{V}[f(x)] = \\int\\left(f(x) - \\mathbb{E}[f(x)]\\right)^2p(x)dx,\n",
    "$$\n",
    "2. and the probability of failure:\n",
    "$$\n",
    "p_{\\mbox{fail}} = P[y > 1.2] = \\int\\mathcal{X}_{[1.2,+\\infty)}(f(x))p(x)dx,\n",
    "$$\n",
    "where $\\mathcal{X}_A$ is the characteristic function of the set A, i.e., $\\mathcal{X}_A(x) = 1$ if $x\\in A$ and $\\mathcal{X}_A(x) = 0$ otherwise.\n",
    "Unfortunately, our boss is not very happy with our performance. He is going to shut down the project unless we have an answer in ten days. However, a single simulation takes a day... We can only do 10 simulations! What do we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the \"amazing code\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Here is an amazing code:\n",
    "solver = lambda(x): -np.cos(np.pi * x) + np.sin(4. * np.pi * x)\n",
    "# It accepts just one input parameter that varies between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Learning About GP Regression\n",
    "This demonstrates how do do Gaussian process regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "import cPickle as pickle\n",
    "import GPy\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(1345678)\n",
    "\n",
    "# Select the number of simulations you want to perform:\n",
    "num_sim = 10\n",
    "\n",
    "# Generate the input data (needs to be column matrix)\n",
    "X = np.random.rand(num_sim, 1)\n",
    "\n",
    "# Evaluate our amazing code at these points:\n",
    "Y = solver(X)\n",
    "\n",
    "# Pick a covariance function\n",
    "k = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\n",
    "\n",
    "# Construct the GP regression model\n",
    "m = GPy.models.GPRegression(X, Y, k)\n",
    "\n",
    "# That's it. Print some details about the model:\n",
    "print m\n",
    "\n",
    "# Now we would like to make some predictions\n",
    "# Namely, we wish to predict at this dense set of points:\n",
    "X_p = np.linspace(0, 1., 100)[:, None]\n",
    "\n",
    "# We can make predictions as follows\n",
    "Y_p, V_p = m.predict(X_p) # Y_p = mean prediction, V_p = predictive variance\n",
    "# Here is the standard deviation:\n",
    "S_p = np.sqrt(V_p)\n",
    "# Lower predictive bound\n",
    "Y_l = Y_p - 2. * S_p\n",
    "# Upper predictive bound\n",
    "Y_u = Y_p + 2. * S_p\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_p, Y_p, label='Predictive mean')\n",
    "ax.fill_between(X_p.flatten(), Y_l.flatten(), Y_u.flatten(), alpha=0.25, label='Predictive error bars')\n",
    "ax.plot(X, Y, 'kx', markeredgewidth=2, label='Observed data')\n",
    "\n",
    "# Write the model to a file \n",
    "print '> writing model to file: surrogate.pcl'\n",
    "#with open('surrogate.pcl', 'wb') as fd:\n",
    "#    pickle.dump(m, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. The fit looks pretty bad. Why do you think that is? Are our prior assumptions about the parameters of the GP compatible with reality?\n",
    "\n",
    "2. Ok. We know that our code is deterministic but the GP thinks that there is noise there. Let’s fix this. Go to line 40 and type:\n",
    "```\n",
    "   m.likelihood.variance = 0\n",
    "```\n",
    "This tells the GP that the observations have no noise. Rerun the code. Is the fit better?\n",
    "3. The previous question was not supposed to work. Why do you think it failed? It\n",
    "can be fixed by making the variance something small, e.g., make it 1e-6 instead of exactly zero. Rerun the code. Is the fit now any better?\n",
    "4. We are not quite there. The length scale we are using is 1. Perhaps our function is not that smooth. Try to pick a more reasonable value for the length scale and rerun the code. What do you think is a good value?\n",
    "5. Repeat 3 for the variance parameter of the SE covariance function.\n",
    "6. That’s too painful and not very scientific. The proper way to find the parameters is to maximize the likelihood. Undo the modifications you made so far and type ```m.optimize()``` after the model definition.\n",
    "\n",
    "This maximizes the marginal likelihood of your model using the BFGS algorithm and honoring any constraints. Rerun the examples. What are the parameters that the algorithm finds? Do they make sense? How do the results look like?\n",
    "7. Based on the results you obtained in 5, we decide to ask our boss for one more\n",
    "day. We believe that doing one more simulation will greatly reduce error in our predictions. At which input point you think we should make this simulation? You can augement the input data by typing:\n",
    "```\n",
    "   X = np.vstack([X, [[0.7]]])\n",
    "```\n",
    "where, of course, you should replace “0.7” with the point you think is the best. This just appends a new input point to the existing X. Rerun the example. What fit do you get now? \n",
    "8. If you are this fast, try repeating 5-6 with a less smooth covariance function, e.g.,\n",
    "the Matern32. What do you observe? Is the prediction uncertainty larger or smaller?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
